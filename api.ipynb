{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai langchain PyMuPDF tensorflow-hub tensorflow sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "import fitz\n",
    "import numpy as np\n",
    "import openai\n",
    "import tensorflow_hub as hub\n",
    "from fastapi import UploadFile\n",
    "from lcserve import serving\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "recommender = None\n",
    "\n",
    "\n",
    "def download_pdf(url, output_path):\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def pdf_to_text(path, start_page=1, end_page=None):\n",
    "    doc = fitz.open(path)\n",
    "    total_pages = doc.page_count\n",
    "\n",
    "    if end_page is None:\n",
    "        end_page = total_pages\n",
    "\n",
    "    text_list = []\n",
    "\n",
    "    for i in range(start_page - 1, end_page):\n",
    "        text = doc.load_page(i).get_text(\"text\")\n",
    "        text = preprocess(text)\n",
    "        text_list.append(text)\n",
    "\n",
    "    doc.close()\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def text_to_chunks(texts, word_length=150, start_page=1):\n",
    "    text_toks = [t.split(' ') for t in texts]\n",
    "    page_nums = []\n",
    "    chunks = []\n",
    "\n",
    "    for idx, words in enumerate(text_toks):\n",
    "        for i in range(0, len(words), word_length):\n",
    "            chunk = words[i: i + word_length]\n",
    "            if (\n",
    "                (i + word_length) > len(words)\n",
    "                and (len(chunk) < word_length)\n",
    "                and (len(text_toks) != (idx + 1))\n",
    "            ):\n",
    "                text_toks[idx + 1] = chunk + text_toks[idx + 1]\n",
    "                continue\n",
    "            chunk = ' '.join(chunk).strip()\n",
    "            chunk = f'[Page no. {idx+start_page}]' + ' ' + '\"' + chunk + '\"'\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "class SemanticSearch:\n",
    "    def __init__(self):\n",
    "        self.use = hub.load(\n",
    "            'https://tfhub.dev/google/universal-sentence-encoder/4')\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, data, batch=1000, n_neighbors=5):\n",
    "        self.data = data\n",
    "        self.embeddings = self.get_text_embedding(data, batch=batch)\n",
    "        n_neighbors = min(n_neighbors, len(self.embeddings))\n",
    "        self.nn = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "        self.nn.fit(self.embeddings)\n",
    "        self.fitted = True\n",
    "\n",
    "    def __call__(self, text, return_data=True):\n",
    "        inp_emb = self.use([text])\n",
    "        neighbors = self.nn.kneighbors(inp_emb, return_distance=False)[0]\n",
    "\n",
    "        if return_data:\n",
    "            return [self.data[i] for i in neighbors]\n",
    "        else:\n",
    "            return neighbors\n",
    "\n",
    "    def get_text_embedding(self, texts, batch=1000):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch):\n",
    "            text_batch = texts[i: (i + batch)]\n",
    "            emb_batch = self.use(text_batch)\n",
    "            embeddings.append(emb_batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_recommender(path, start_page=1):\n",
    "    global recommender\n",
    "    if recommender is None:\n",
    "        recommender = SemanticSearch()\n",
    "\n",
    "    texts = pdf_to_text(path, start_page=start_page)\n",
    "    chunks = text_to_chunks(texts, start_page=start_page)\n",
    "    recommender.fit(chunks)\n",
    "    return \"Corpus Loaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def embed_file(file: UploadFile):\n",
    "    suffix = Path(file.filename).suffix\n",
    "    with NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n",
    "        shutil.copyfileobj(file.file, tmp)\n",
    "        tmp_path = Path(tmp.name)\n",
    "\n",
    "    load_recommender(str(tmp_path))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
